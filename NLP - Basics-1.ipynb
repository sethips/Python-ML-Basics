{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.NLTK and the Basics - Counting Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\750010524\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "#nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words\n",
    "md = nltk.corpus.gutenberg.words(\"melville-moby_dick.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'Moby',\n",
       " 'Dick',\n",
       " 'by',\n",
       " 'Herman',\n",
       " 'Melville',\n",
       " '1851',\n",
       " ']',\n",
       " 'ETYMOLOGY',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first ten words\n",
    "md[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "906"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of a word\n",
    "md.count('whale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of a word\n",
    "md.count(\"mallesh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260819"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of words\n",
    "len(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique words\n",
    "md_set = set(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19317"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of sentences\n",
    "md_sents = nltk.corpus.gutenberg.sents(\"melville-moby_dick.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\750010524\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10059"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']'],\n",
       " ['ETYMOLOGY', '.']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_sents[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Words Per Sentence Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     C:\\Users\\750010524\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('inaugural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt',\n",
       " '1829-Jackson.txt',\n",
       " '1833-Jackson.txt',\n",
       " '1837-VanBuren.txt',\n",
       " '1841-Harrison.txt',\n",
       " '1845-Polk.txt',\n",
       " '1849-Taylor.txt',\n",
       " '1853-Pierce.txt',\n",
       " '1857-Buchanan.txt',\n",
       " '1861-Lincoln.txt',\n",
       " '1865-Lincoln.txt',\n",
       " '1869-Grant.txt',\n",
       " '1873-Grant.txt',\n",
       " '1877-Hayes.txt',\n",
       " '1881-Garfield.txt',\n",
       " '1885-Cleveland.txt',\n",
       " '1889-Harrison.txt',\n",
       " '1893-Cleveland.txt',\n",
       " '1897-McKinley.txt',\n",
       " '1901-McKinley.txt',\n",
       " '1905-Roosevelt.txt',\n",
       " '1909-Taft.txt',\n",
       " '1913-Wilson.txt',\n",
       " '1917-Wilson.txt',\n",
       " '1921-Harding.txt',\n",
       " '1925-Coolidge.txt',\n",
       " '1929-Hoover.txt',\n",
       " '1933-Roosevelt.txt',\n",
       " '1937-Roosevelt.txt',\n",
       " '1941-Roosevelt.txt',\n",
       " '1945-Roosevelt.txt',\n",
       " '1949-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1965-Johnson.txt',\n",
       " '1969-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1977-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '2001-Bush.txt',\n",
       " '2005-Bush.txt',\n",
       " '2009-Obama.txt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When',\n",
       " 'it',\n",
       " 'was',\n",
       " 'first',\n",
       " 'perceived',\n",
       " ',',\n",
       " 'in',\n",
       " 'early',\n",
       " 'times',\n",
       " ',',\n",
       " 'that',\n",
       " 'no',\n",
       " 'middle',\n",
       " 'course',\n",
       " 'for',\n",
       " 'America',\n",
       " 'remained',\n",
       " 'between',\n",
       " 'unlimited',\n",
       " 'submission']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural.words('1797-Adams.txt')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538 1789-Washington.txt\n",
      "147 1793-Washington.txt\n",
      "2585 1797-Adams.txt\n",
      "1935 1801-Jefferson.txt\n",
      "2384 1805-Jefferson.txt\n",
      "1265 1809-Madison.txt\n",
      "1304 1813-Madison.txt\n",
      "3693 1817-Monroe.txt\n",
      "4909 1821-Monroe.txt\n",
      "3150 1825-Adams.txt\n",
      "1208 1829-Jackson.txt\n",
      "1267 1833-Jackson.txt\n",
      "4171 1837-VanBuren.txt\n",
      "9165 1841-Harrison.txt\n",
      "5196 1845-Polk.txt\n",
      "1182 1849-Taylor.txt\n",
      "3657 1853-Pierce.txt\n",
      "3098 1857-Buchanan.txt\n",
      "4005 1861-Lincoln.txt\n",
      "785 1865-Lincoln.txt\n",
      "1239 1869-Grant.txt\n",
      "1478 1873-Grant.txt\n",
      "2724 1877-Hayes.txt\n",
      "3239 1881-Garfield.txt\n",
      "1828 1885-Cleveland.txt\n",
      "4750 1889-Harrison.txt\n",
      "2153 1893-Cleveland.txt\n",
      "4371 1897-McKinley.txt\n",
      "2450 1901-McKinley.txt\n",
      "1091 1905-Roosevelt.txt\n",
      "5846 1909-Taft.txt\n",
      "1905 1913-Wilson.txt\n",
      "1656 1917-Wilson.txt\n",
      "3756 1921-Harding.txt\n",
      "4442 1925-Coolidge.txt\n",
      "3890 1929-Hoover.txt\n",
      "2063 1933-Roosevelt.txt\n",
      "2019 1937-Roosevelt.txt\n",
      "1536 1941-Roosevelt.txt\n",
      "637 1945-Roosevelt.txt\n",
      "2528 1949-Truman.txt\n",
      "2775 1953-Eisenhower.txt\n",
      "1917 1957-Eisenhower.txt\n",
      "1546 1961-Kennedy.txt\n",
      "1715 1965-Johnson.txt\n",
      "2425 1969-Nixon.txt\n",
      "2028 1973-Nixon.txt\n",
      "1380 1977-Carter.txt\n",
      "2801 1981-Reagan.txt\n",
      "2946 1985-Reagan.txt\n",
      "2713 1989-Bush.txt\n",
      "1855 1993-Clinton.txt\n",
      "2462 1997-Clinton.txt\n",
      "1825 2001-Bush.txt\n",
      "2376 2005-Bush.txt\n",
      "2726 2009-Obama.txt\n"
     ]
    }
   ],
   "source": [
    "for speech in inaugural.fileids():\n",
    "    words_total = len(inaugural.words(speech))\n",
    "    print(words_total, speech)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_len = [(len(inaugural.words(speech)), speech) for speech in inaugural.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9165, '1841-Harrison.txt')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(speech_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = nltk.corpus.gutenberg.words('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency distribution takes a list of words and counts how many times each word is seen.\n",
    "alicefd = nltk.FreqDist(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 1993, \"'\": 1731, 'the': 1527, 'and': 802, '.': 764, 'to': 725, 'a': 615, 'I': 543, 'it': 527, 'she': 509, ...})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alicefd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alicefd[\"Mallesh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1527"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alicefd[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 1993),\n",
       " (\"'\", 1731),\n",
       " ('the', 1527),\n",
       " ('and', 802),\n",
       " ('.', 764),\n",
       " ('to', 725),\n",
       " ('a', 615),\n",
       " ('I', 543),\n",
       " ('it', 527),\n",
       " ('she', 509),\n",
       " ('of', 500),\n",
       " ('said', 456),\n",
       " (\",'\", 397),\n",
       " ('Alice', 396),\n",
       " ('in', 357),\n",
       " ('was', 352),\n",
       " ('you', 345),\n",
       " (\"!'\", 278),\n",
       " ('that', 275),\n",
       " ('as', 246)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20 most common words\n",
    "alicefd.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lewis',\n",
       " 'Carroll',\n",
       " '1865',\n",
       " ']',\n",
       " 'Hole',\n",
       " 'conversations',\n",
       " 'daisy',\n",
       " 'chain',\n",
       " 'daisies',\n",
       " 'pink']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A word used only once in a collection of text is called a hapax legomenon\n",
    "alicefd.hapaxes()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conditional Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A conditional frequency distribution counts multiple cases or conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = [(\"USA\", \"NY\" ), (\"USA\", \"AZ\"), (\"USA\", \"NY\"), (\"INDIA\", \"TS\"), (\"INDIA\", \"MH\"), (\"INDIA\", \"MH\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('USA', 'NY'),\n",
       " ('USA', 'AZ'),\n",
       " ('USA', 'NY'),\n",
       " ('INDIA', 'TS'),\n",
       " ('INDIA', 'MH'),\n",
       " ('INDIA', 'MH')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('USA', 'NY'): 2, ('INDIA', 'MH'): 2, ('USA', 'AZ'): 1, ('INDIA', 'TS'): 1})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Running a regular frequency distribution on the list..\n",
    "nltk.FreqDist(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Informative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st book\n",
    "alice = nltk.corpus.gutenberg.words('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_fd = nltk.FreqDist(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the 100 most common words.\n",
    "alice_fd_100 = alice_fd.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd book\n",
    "moby = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
    "moby_fd = nltk.FreqDist(moby)\n",
    "moby_fd_100 = moby_fd.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_100 = [word[0] for word in alice_fd_100]\n",
    "moby_100  = [word[0]  for word in moby_fd_100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('all', 173),\n",
       " ('had', 177),\n",
       " ('me', 61),\n",
       " ('say', 51),\n",
       " ('in', 357),\n",
       " (\".'\", 187),\n",
       " ('or', 76),\n",
       " ('was', 352),\n",
       " ('be', 145),\n",
       " ('began', 58),\n",
       " ('*', 60),\n",
       " ('for', 140),\n",
       " ('there', 65),\n",
       " ('to', 725),\n",
       " ('an', 52),\n",
       " ('that', 275),\n",
       " ('way', 56),\n",
       " ('my', 55),\n",
       " ('if', 78),\n",
       " ('you', 345),\n",
       " ('would', 70),\n",
       " ('as', 246),\n",
       " ('off', 62),\n",
       " ('they', 129),\n",
       " ('on', 189),\n",
       " ('the', 1527),\n",
       " ('and', 802),\n",
       " ('much', 51),\n",
       " ('can', 57),\n",
       " ('herself', 83),\n",
       " ('no', 69),\n",
       " ('did', 60),\n",
       " ('but', 133),\n",
       " ('s', 195),\n",
       " ('it', 527),\n",
       " ('Gryphon', 55),\n",
       " ('a', 615),\n",
       " ('have', 73),\n",
       " ('And', 67),\n",
       " (\"?'\", 154),\n",
       " ('into', 67),\n",
       " ('very', 126),\n",
       " (\"'\", 1731),\n",
       " ('when', 69),\n",
       " ('not', 129),\n",
       " (':', 216),\n",
       " ('so', 124),\n",
       " ('--', 140),\n",
       " ('Hatter', 55),\n",
       " ('see', 66),\n",
       " ('at', 202),\n",
       " ('then', 72),\n",
       " ('King', 61),\n",
       " ('her', 243),\n",
       " ('know', 87),\n",
       " ('Alice', 396),\n",
       " ('is', 97),\n",
       " ('time', 68),\n",
       " ('of', 500),\n",
       " ('ll', 56),\n",
       " ('his', 94),\n",
       " ('Queen', 74),\n",
       " ('thought', 74),\n",
       " ('t', 216),\n",
       " ('she', 509),\n",
       " ('one', 94),\n",
       " ('It', 64),\n",
       " ('were', 85),\n",
       " ('The', 108),\n",
       " ('them', 88),\n",
       " ('Mock', 56),\n",
       " ('.', 764),\n",
       " ('up', 98),\n",
       " ('could', 73),\n",
       " ('-', 141),\n",
       " (';', 186),\n",
       " ('down', 99),\n",
       " ('your', 53),\n",
       " ('what', 93),\n",
       " ('do', 68),\n",
       " ('with', 175),\n",
       " ('I', 543),\n",
       " ('again', 83),\n",
       " (\"!'\", 278),\n",
       " ('out', 116),\n",
       " (\",'\", 397),\n",
       " ('this', 113),\n",
       " (',', 1993),\n",
       " ('he', 101),\n",
       " ('m', 58),\n",
       " ('said', 456),\n",
       " ('its', 56),\n",
       " ('like', 84),\n",
       " ('Turtle', 59),\n",
       " ('went', 83),\n",
       " ('!', 155),\n",
       " ('about', 94),\n",
       " ('quite', 53),\n",
       " ('little', 125),\n",
       " ('by', 55)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Two sets can be subtracted from one another leaving us with the difference.\n",
    "list(set(alice_fd_100) - set(moby_fd_100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gryphon',\n",
       " ':',\n",
       " 'your',\n",
       " 'off',\n",
       " 'say',\n",
       " '*',\n",
       " 'quite',\n",
       " 'much',\n",
       " 'do',\n",
       " 'went',\n",
       " 'could',\n",
       " 'thought',\n",
       " 'did',\n",
       " 'm',\n",
       " 'Alice',\n",
       " 'Queen',\n",
       " 't',\n",
       " 'she',\n",
       " 'Turtle',\n",
       " 'King',\n",
       " 'know',\n",
       " 'Hatter',\n",
       " \".'\",\n",
       " 'herself',\n",
       " 'Mock',\n",
       " 'can',\n",
       " \"?'\",\n",
       " 'again',\n",
       " 'way',\n",
       " 'said',\n",
       " 'see',\n",
       " 'll',\n",
       " 'began',\n",
       " 'little',\n",
       " \",'\",\n",
       " \"!'\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Two sets can be subtracted from one another leaving us with the difference.\n",
    "list(set(alice_100) - set(moby_100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bigrams, sometimes called 2grams, or ngrams (when dealing with a different number), is a way of looking at word sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'i love NewYork and Mumbai.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'NewYork', 'and', 'Mumbai', '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.bigrams(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 'love')\n",
      "('love', 'NewYork')\n",
      "('NewYork', 'and')\n",
      "('and', 'Mumbai')\n",
      "('Mumbai', '.')\n"
     ]
    }
   ],
   "source": [
    "for item in bigrams:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = nltk.trigrams(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 'love', 'NewYork')\n",
      "('love', 'NewYork', 'and')\n",
      "('NewYork', 'and', 'Mumbai')\n",
      "('and', 'Mumbai', '.')\n"
     ]
    }
   ],
   "source": [
    "for item in trigrams:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'Morgan Stanley Genpact USA INDIA.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Morgan', 'Stanley', 'Genpact', 'USA', 'INDIA', '.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens1 = nltk.word_tokenize(text1)\n",
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams1 = ngrams(tokens1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Morgan', 'Stanley')\n",
      "('Stanley', 'Genpact')\n",
      "('Genpact', 'USA')\n",
      "('USA', 'INDIA')\n",
      "('INDIA', '.')\n"
     ]
    }
   ],
   "source": [
    "for item in bigrams1:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams1 = ngrams(tokens1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Morgan', 'Stanley', 'Genpact')\n",
      "('Stanley', 'Genpact', 'USA')\n",
      "('Genpact', 'USA', 'INDIA')\n",
      "('USA', 'INDIA', '.')\n"
     ]
    }
   ],
   "source": [
    "for item in trigrams1:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "def n_grams(text,n):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    grams  = ngrams(tokens, n)\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Data Sceince NLP AI ML DL.' \n",
    "grams = n_grams(text,5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Data', 'Sceince', 'NLP', 'AI', 'ML')\n",
      "('Sceince', 'NLP', 'AI', 'ML', 'DL')\n",
      "('NLP', 'AI', 'ML', 'DL', '.')\n"
     ]
    }
   ],
   "source": [
    "for item in grams:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = nltk.corpus.gutenberg.words(\"carroll-alice.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'new', 'newspapers'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words starts with 'new'\n",
    "set([word for word in alice if re.search(\"^new\", word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Beautiful',\n",
       " 'barrowful',\n",
       " 'beautiful',\n",
       " 'delightful',\n",
       " 'doubtful',\n",
       " 'dreadful',\n",
       " 'graceful',\n",
       " 'hopeful',\n",
       " 'mournful',\n",
       " 'ootiful',\n",
       " 'respectful',\n",
       " 'sorrowful',\n",
       " 'truthful',\n",
       " 'useful',\n",
       " 'wonderful'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words ends with 'ful'\n",
    "set([word for word in alice if re.search(\"ful$\", word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tunnel', 'dinner', 'fanned', 'dinner', 'manner', 'manner', 'cannot']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words that are six characters long and have two n's in the middle.\n",
    "[word for word in alice if re.search(\"^..nn..$\", word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat', 'hat', 'rat'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words that start with \"c\", \"h\", and \"t\", and end in \"at\".\n",
    "set([word for word in alice if re.search(\"^[chr]at$\", word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ann',\n",
       " 'Dinn',\n",
       " 'Pennyworth',\n",
       " 'annoy',\n",
       " 'annoyed',\n",
       " 'beginning',\n",
       " 'cannot',\n",
       " 'cunning',\n",
       " 'dinn',\n",
       " 'dinner',\n",
       " 'fanned',\n",
       " 'fanning',\n",
       " 'funny',\n",
       " 'grinned',\n",
       " 'grinning',\n",
       " 'manner',\n",
       " 'manners',\n",
       " 'planning',\n",
       " 'running',\n",
       " 'tunnel'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words of any length that have two n's.\n",
    "set([word for word in alice if re.search(\"^.*nn.*$\", word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
