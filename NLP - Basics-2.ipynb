{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SHR', 'is', 'my', 'favorite', 'team', 'in', 'IPL']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = 'SHR is my favorite team in IPL'\n",
    "tokens = nltk.word_tokenize(txt)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SHR is my favorite team in IPL.', 'i love playing cricket']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt2 = 'SHR is my favorite team in IPL. i love playing cricket'\n",
    "tokens2 = nltk.sent_tokenize(txt2)\n",
    "tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SHR', 'is', 'my', 'favorite', 'team', 'in', 'IPL', '.']\n",
      "['i', 'love', 'playing', 'cricket']\n"
     ]
    }
   ],
   "source": [
    "#tokenize our sentence tokens.\n",
    "for item in tokens2:\n",
    "    print(nltk.word_tokenize(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing punctuation.\n",
    "w1 = nltk.corpus.gutenberg.words(\"melville-moby_dick.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'Moby',\n",
       " 'Dick',\n",
       " 'by',\n",
       " 'Herman',\n",
       " 'Melville',\n",
       " '1851',\n",
       " ']',\n",
       " 'ETYMOLOGY',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1_10 = w1[:10]\n",
    "w1_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "moby\n",
      "dick\n",
      "by\n",
      "herman\n",
      "melville\n",
      "1851\n",
      "]\n",
      "etymology\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Making everything lower case.\n",
    "for word in w1_10:\n",
    "    print(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moby', 'dick', 'by', 'herman', 'melville', 'etymology']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = [word.lower() for word in w1_10 if word.isalpha()]\n",
    "norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemmers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Stemmers help further normalize text when we run into words that might be plural. There are many different kinds of stemmers  available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_words = [\"cat\",\"cats\",\"lie\",\"lying\",\"run\",\"running\",\"city\",\"cities\",\"month\",\"monthly\",\"woman\",\"women\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cat\n",
      "lie\n",
      "lie\n",
      "run\n",
      "run\n",
      "citi\n",
      "citi\n",
      "month\n",
      "monthli\n",
      "woman\n",
      "women\n"
     ]
    }
   ],
   "source": [
    "for word in my_words:\n",
    "    print(porter.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cat\n",
      "lie\n",
      "lying\n",
      "run\n",
      "run\n",
      "city\n",
      "city\n",
      "mon\n",
      "month\n",
      "wom\n",
      "wom\n"
     ]
    }
   ],
   "source": [
    "for word in my_words:\n",
    "    print(lancaster.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\750010524\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnlem = nltk.WordNetLemmatizer()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cat\n",
      "lie\n",
      "lying\n",
      "run\n",
      "running\n",
      "city\n",
      "city\n",
      "month\n",
      "monthly\n",
      "woman\n",
      "woman\n"
     ]
    }
   ],
   "source": [
    "for word in my_words:\n",
    "    print(wnlem.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### A part of speech tagger will identify the part of speech for a sequence of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\750010524\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\750010524\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'i Take PATH train to go to work in Manhatten.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens =  nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'Take',\n",
       " 'PATH',\n",
       " 'train',\n",
       " 'to',\n",
       " 'go',\n",
       " 'to',\n",
       " 'work',\n",
       " 'in',\n",
       " 'Manhatten',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NNS'),\n",
       " ('Take', 'VBP'),\n",
       " ('PATH', 'NNP'),\n",
       " ('train', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('go', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('work', 'VB'),\n",
       " ('in', 'IN'),\n",
       " ('Manhatten', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'), ('love', 'VBP'), ('NYC', 'NN')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(nltk.word_tokenize('i love NYC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of all nouns\n",
    "md = nltk.corpus.gutenberg.words(\"melville-moby_dick.txt\")\n",
    "md_norm = [word.lower() for word in md if word.isalpha()]\n",
    "md_tag = nltk.pos_tag(md_norm, tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('moby', 'NOUN'),\n",
       " ('dick', 'NOUN'),\n",
       " ('by', 'ADP'),\n",
       " ('herman', 'NOUN'),\n",
       " ('melville', 'NOUN'),\n",
       " ('etymology', 'NOUN'),\n",
       " ('supplied', 'VERB'),\n",
       " ('by', 'ADP'),\n",
       " ('a', 'DET'),\n",
       " ('late', 'ADJ')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_tag[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_nouns = [word[0] for word in md_tag if word[1] == \"NOUN\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_fd = nltk.FreqDist(md_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 1182),\n",
       " ('whale', 909),\n",
       " ('s', 774),\n",
       " ('man', 527),\n",
       " ('ship', 498),\n",
       " ('sea', 435),\n",
       " ('head', 337),\n",
       " ('time', 334),\n",
       " ('boat', 332),\n",
       " ('ahab', 278)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns_fd.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Parts of Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Words can be tagged with a different part of speech based on usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice      = nltk.corpus.gutenberg.words(\"carroll-alice.txt\")\n",
    "alice_norm = [word.lower() for word in alice if word.isalpha()]\n",
    "alice_tags = nltk.pos_tag(alice_norm, tagset='universal')\n",
    "alice_cfd   = nltk.ConditionalFreqDist(alice_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'NOUN': 5, 'VERB': 3, 'ADP': 1})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_cfd['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'NOUN': 11})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_cfd['book']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'ADP': 31, 'PRT': 5, 'ADV': 4})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_cfd['over']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Finding all the cases in a given text where there was a choice between two options, \"NOUN 'or' NOUN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = nltk.corpus.gutenberg.words(\"bryant-stories.txt\")\n",
    "tags = nltk.pos_tag(stories, tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[', 'NOUN'),\n",
       " ('Stories', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('Tell', 'VERB'),\n",
       " ('to', 'PRT'),\n",
       " ('Children', 'NOUN'),\n",
       " ('by', 'ADP'),\n",
       " ('Sara', 'NOUN'),\n",
       " ('Cone', 'NOUN'),\n",
       " ('Bryant', 'NOUN')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship or part\n",
      "food or water\n",
      "queens or princesses\n",
      "rank or wealth\n"
     ]
    }
   ],
   "source": [
    "for ((word1, tag1), (word2, tag2), (word3, tag3)) in nltk.trigrams(tags):\n",
    "    if tag1 == 'NOUN' and word2 == 'or' and tag3 =='NOUN':\n",
    "        print(word1 + \" \" + word2 + \" \" + word3)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Through chunking, we can prevent two word entities from being split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I will go to the coffee shop in New York after I get off the jet plane.\"\n",
    "sent_tag = nltk.pos_tag(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('go', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('coffee', 'NN'),\n",
       " ('shop', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('New', 'NNP'),\n",
       " ('York', 'NNP'),\n",
       " ('after', 'IN'),\n",
       " ('I', 'PRP'),\n",
       " ('get', 'VBP'),\n",
       " ('off', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('jet', 'NN'),\n",
       " ('plane', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence =  '''\n",
    "            CHUNK: {<NNP>+}\n",
    "                   {<NN>+}\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "NPChunker = nltk.RegexpParser(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = NPChunker.parse(sent_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  will/MD\n",
      "  go/VB\n",
      "  to/TO\n",
      "  the/DT\n",
      "  (CHUNK coffee/NN shop/NN)\n",
      "  in/IN\n",
      "  (CHUNK New/NNP York/NNP)\n",
      "  after/IN\n",
      "  I/PRP\n",
      "  get/VBP\n",
      "  off/IN\n",
      "  the/DT\n",
      "  (CHUNK jet/NN plane/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\750010524\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\750010524\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('example.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'World War II (WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945, though related conflicts began earlier. It involved the vast majority of the world\\'s nationsâ€”including all of the great powersâ€”eventually forming two opposing military alliances: the Allies and the Axis. It was the most widespread war in history, and directly involved more than 100 million people from over 30 countries. In a state of \"total war\", the major participants threw their entire economic, industrial, and scientific capabilities behind the war effort, erasing the distinction between civilian and military resources. Marked by mass deaths of civilians, including the Holocaust (in which approximately 11 million people were killed) and the strategic bombing of industrial and population centres (in which approximately one million were killed, and which included the atomic bombings of Hiroshima and Nagasaki), it resulted in an estimated 50 million to 85 million fatalities. These made World War II the deadliest conflict in human history.\\n\\nThe Empire of Japan aimed to dominate Asia and the Pacific and was already at war with the Republic of China in 1937, but the world war is generally said to have begun on 1 September 1939 with the invasion of Poland by Germany and subsequent declarations of war on Germany by France and the United Kingdom. From late 1939 to early 1941, in a series of campaigns and treaties, Germany conquered or controlled much of continental Europe, and formed the Axis alliance with Italy and Japan. Based on the Molotovâ€“Ribbentrop Pact of August 1939, Germany and the Soviet Union partitioned and annexed territories of their European neighbours, Poland, Finland, Romania and the Baltic states. For a year starting in late June 1940, the United Kingdom and the British Commonwealth were the only Allied forces continuing the fight against the European Axis powers, with campaigns in North Africa and the Horn of Africa, the aerial Battle of Britain and the Blitz bombing campaign, as well as the long-running Battle of the Atlantic. In June 1941, the European Axis powers launched an invasion of the Soviet Union, opening the largest land theatre of war in history, which trapped the major part of the Axis\\' military forces into a war of attrition. In December 1941, Japan attacked the United States and European territories in the Pacific Ocean, and quickly conquered much of the Western Pacific.\\n\\nThe Axis advance halted in 1942 when Japan lost the critical Battle of Midway, near Hawaii, and Germany was defeated in North Africa and then, decisively, at Stalingrad in the Soviet Union. In 1943, with a series of German defeats on the Eastern Front, the Allied invasion of Italy which brought about Italian surrender, and Allied victories in the Pacific, the Axis lost the initiative and undertook strategic retreat on all fronts. In 1944, the Western Allies invaded German-occupied France, while the Soviet Union regained all of its territorial losses and invaded Germany and its allies. During 1944 and 1945 the Japanese suffered major reverses in mainland Asia in South Central China and Burma, while the Allies crippled the Japanese Navy and captured key Western Pacific islands.\\n\\nThe war in Europe ended with an invasion of Germany by the Western Allies and the Soviet Union culminating in the capture of Berlin by Soviet and Polish troops and the subsequent German unconditional surrender on 8 May 1945. Following the Potsdam Declaration by the Allies on 26 July 1945 and the refusal of Japan to surrender under its terms, the United States dropped atomic bombs on the Japanese cities of Hiroshima and Nagasaki on 6 August and 9 August respectively. With an invasion of the Japanese archipelago imminent, the possibility of additional atomic bombings, and the Soviet Union\\'s declaration of war on Japan and invasion of Manchuria, Japan surrendered on 15 August 1945. Thus ended the war in Asia, cementing the total victory of the Allies.\\n\\nWorld War II altered the political alignment and social structure of the world. The United Nations (UN) was established to foster international co-operation and prevent future conflicts. The victorious great powersâ€”the United States, the Soviet Union, China, the United Kingdom, and Franceâ€”became the permanent members of the United Nations Security Council. The Soviet Union and the United States emerged as rival superpowers, setting the stage for the Cold War, which lasted for the next 46 years. Meanwhile, the influence of European great powers waned, while the decolonisation of Asia and Africa began. Most countries whose industries had been damaged moved towards economic recovery. Political integration, especially in Europe, emerged as an effort to end pre-war enmities and to create a common identity.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tag = nltk.pos_tag(nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ch = nltk.ne_chunk(text_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORGANIZATION WWII\n",
      "ORGANIZATION WW2\n",
      "ORGANIZATION Second\n",
      "ORGANIZATION Axis\n",
      "ORGANIZATION Hiroshima\n",
      "GPE Nagasaki\n",
      "ORGANIZATION Empire of Japan\n",
      "GPE Asia\n",
      "ORGANIZATION Pacific\n",
      "ORGANIZATION Republic\n",
      "GPE China\n",
      "GPE Poland\n",
      "GPE Germany\n",
      "GPE Germany\n",
      "GPE France\n",
      "ORGANIZATION United Kingdom\n",
      "GPE Germany\n",
      "GPE Europe\n",
      "ORGANIZATION Axis\n",
      "GPE Italy\n",
      "GPE Japan\n",
      "GPE Germany\n",
      "GPE Soviet Union\n",
      "GPE European\n",
      "GPE Poland\n",
      "GPE Finland\n",
      "GPE Romania\n",
      "GPE Baltic\n",
      "ORGANIZATION United Kingdom\n",
      "GPE British\n",
      "ORGANIZATION European Axis\n",
      "GPE North Africa\n",
      "ORGANIZATION Horn\n",
      "GPE Africa\n",
      "GPE Britain\n",
      "GPE Blitz\n",
      "ORGANIZATION Atlantic\n",
      "ORGANIZATION European Axis\n",
      "GPE Soviet Union\n",
      "ORGANIZATION Axis\n",
      "GPE Japan\n",
      "GPE United States\n",
      "GPE European\n",
      "ORGANIZATION Pacific Ocean\n",
      "LOCATION Western Pacific\n",
      "ORGANIZATION Axis\n",
      "PERSON Japan\n",
      "GPE Midway\n",
      "GPE Hawaii\n",
      "GPE Germany\n",
      "GPE North Africa\n",
      "FACILITY Stalingrad\n",
      "GPE Soviet Union\n",
      "GPE German\n",
      "LOCATION Eastern Front\n",
      "GPE Italy\n",
      "GPE Italian\n",
      "GPE Allied\n",
      "ORGANIZATION Pacific\n",
      "ORGANIZATION Axis\n",
      "LOCATION Western\n",
      "GPE France\n",
      "GPE Soviet Union\n",
      "GPE Germany\n",
      "GPE Japanese\n",
      "GPE Asia\n",
      "GPE South\n",
      "GPE China\n",
      "GPE Burma\n",
      "GPE Japanese\n",
      "ORGANIZATION Navy\n",
      "LOCATION Western Pacific\n",
      "GPE Europe\n",
      "GPE Germany\n",
      "LOCATION Western\n",
      "GPE Soviet Union\n",
      "GPE Berlin\n",
      "GPE Soviet\n",
      "GPE Polish\n",
      "GPE German\n",
      "ORGANIZATION Potsdam\n",
      "GPE Japan\n",
      "GPE United States\n",
      "GPE Japanese\n",
      "ORGANIZATION Hiroshima\n",
      "PERSON Nagasaki\n",
      "GPE Japanese\n",
      "GPE Soviet Union\n",
      "GPE Japan\n",
      "GPE Manchuria\n",
      "GPE Japan\n",
      "GPE Asia\n",
      "ORGANIZATION United Nations\n",
      "GPE United States\n",
      "GPE Soviet Union\n",
      "GPE China\n",
      "ORGANIZATION United Kingdom\n",
      "ORGANIZATION United Nations\n",
      "ORGANIZATION Security Council\n",
      "GPE Soviet Union\n",
      "GPE United States\n",
      "GPE European\n",
      "GPE Asia\n",
      "PERSON Africa\n",
      "GPE Europe\n"
     ]
    }
   ],
   "source": [
    "for chunk in text_ch:\n",
    "    if hasattr(chunk, 'label'):\n",
    "        print(chunk.label(), ' '.join(c[0] for c in chunk.leaves()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
